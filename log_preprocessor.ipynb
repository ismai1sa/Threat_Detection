{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a5b3e0-86e3-4d58-bd74-45b8eb14ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] All logs cleaned, normalized, and saved to normalized_logs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1662/3043133616.py:127: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_dataset = pd.concat(dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Corrected: removed duplicate 'dest_domain'\n",
    "REQUIRED_COLUMNS = [\n",
    "    'timestamp', 'user', 'action', 'description', 'protocol', 'url', 'method',\n",
    "    'source_ip', 'dest_ip', 'dest_domain', 'client_ip', 'source_port',\n",
    "    'dest_port', 'severity', 'host', 'rule_name', 'app', 'category',\n",
    "    'bytes_sent', 'bytes_received', 'threat_id', 'user_agent', 'eventid',\n",
    "    'workstation', 'privilege', 'object', 'accessmask', 'target', 'status',\n",
    "    'reason', 'process', 'parentprocess', 'fileaccessed', 'alert', 'machine',\n",
    "    'os', 'details', 'resource', 'hostname', 'location', 'facility', 'code',\n",
    "    'message', 'event', 'file', 'verdict', 'log_type'\n",
    "]\n",
    "\n",
    "def clean_csv_log(file_path, log_type):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.dropna(how='all', inplace=True)\n",
    "        df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
    "        df['log_type'] = log_type\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
    "\n",
    "def normalize_timestamp(ts):\n",
    "    try:\n",
    "        return datetime.strptime(ts, '%Y-%m-%d %H:%M:%S').isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def clean_ip_column(ip):\n",
    "    ip_pattern = r'^(\\d{1,3}\\.){3}\\d{1,3}$'\n",
    "    if isinstance(ip, str) and re.match(ip_pattern, ip):\n",
    "        return ip\n",
    "    return np.nan\n",
    "\n",
    "def clean_domain_column(domain):\n",
    "    domain_pattern = r'^[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    if isinstance(domain, str) and re.match(domain_pattern, domain):\n",
    "        return domain.lower()\n",
    "    return np.nan\n",
    "\n",
    "def map_ad_columns(df):\n",
    "    ip_fields = ['client_ip', 'client_address', 'ip_address', 'ip']\n",
    "    client_ip_col = next((col for col in ip_fields if col in df.columns), None)\n",
    "    \n",
    "    if client_ip_col:\n",
    "        df['client_ip'] = df[client_ip_col]\n",
    "    else:\n",
    "        df['client_ip'] = None\n",
    "\n",
    "    col_map = {\n",
    "        'user': 'user',\n",
    "        'description': 'description',\n",
    "        'timestamp': 'timestamp'\n",
    "    }\n",
    "    col_map = {k: v for k, v in col_map.items() if k in df.columns}\n",
    "    df = df.rename(columns=col_map)\n",
    "    return df\n",
    "\n",
    "def map_proxy_columns(df):\n",
    "    ip_fields = ['client', 'client_ip', 'source_ip', 'ip']\n",
    "    for field in ip_fields:\n",
    "        if field in df.columns:\n",
    "            df['client_ip'] = df[field].astype(str).str.extract(r'(\\d{1,3}(?:\\.\\d{1,3}){3})')[0]\n",
    "            break\n",
    "    else:\n",
    "        df['client_ip'] = None\n",
    "\n",
    "    if 'url' in df.columns:\n",
    "        df['hostname'] = df['url'].apply(lambda x: urlparse(str(x)).hostname if pd.notnull(x) else None)\n",
    "\n",
    "    return df\n",
    "\n",
    "def map_firewall_columns(df):\n",
    "    if 'dest_ip' in df.columns:\n",
    "        df['dest_domain'] = df['dest_ip'].apply(\n",
    "            lambda x: x if isinstance(x, str) and not re.match(r'^(\\d{1,3}\\.){3}\\d{1,3}$', x) else np.nan\n",
    "        )\n",
    "        df['dest_ip'] = df['dest_ip'].apply(clean_ip_column)\n",
    "    return df\n",
    "\n",
    "# Define your input files\n",
    "log_files = {\n",
    "    \"firewall\": \"./logs/firewall_logs.csv\",\n",
    "    \"proxy\": \"./logs/proxy_logs.csv\",\n",
    "    \"ad\": \"./logs/ad_logs.csv\",\n",
    "    \"edr\": \"./logs/edr_logs.csv\",\n",
    "    \"iam\": \"./logs/iam_logs.csv\",\n",
    "    \"mail\": \"./logs/mail_logs.csv\",\n",
    "    \"router\": \"./logs/router_logs.csv\",\n",
    "    \"user\": \"./logs/user.csv\",\n",
    "    \"siem\": \"./logs/siem_logs.csv\"\n",
    "}\n",
    "\n",
    "dataframes = []\n",
    "for log_type, file_path in log_files.items():\n",
    "    df = clean_csv_log(file_path, log_type)\n",
    "\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    if log_type == 'ad':\n",
    "        df = map_ad_columns(df)\n",
    "    elif log_type == 'proxy':\n",
    "        df = map_proxy_columns(df)\n",
    "    elif log_type == 'firewall':\n",
    "        df = map_firewall_columns(df)\n",
    "    else:\n",
    "        df['client_ip'] = None\n",
    "        df['dest_domain'] = None\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for col in REQUIRED_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    df = df[REQUIRED_COLUMNS]  # Reorder and limit columns\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all processed logs\n",
    "final_dataset = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Normalize timestamps\n",
    "final_dataset['timestamp'] = final_dataset['timestamp'].apply(normalize_timestamp)\n",
    "\n",
    "# Clean IP fields\n",
    "for ip_col in ['source_ip', 'dest_ip', 'client_ip']:\n",
    "    final_dataset[ip_col] = final_dataset[ip_col].apply(clean_ip_column)\n",
    "\n",
    "# Clean domain fields\n",
    "final_dataset['domain'] = final_dataset['hostname'].apply(clean_domain_column)\n",
    "final_dataset['dest_domain'] = final_dataset['dest_domain'].apply(clean_domain_column)\n",
    "\n",
    "# Drop temporary hostname column\n",
    "final_dataset.drop(columns=['hostname'], inplace=True)\n",
    "\n",
    "# Save final output\n",
    "final_dataset.to_csv(\"normalized_logs.csv\", index=False)\n",
    "print(\"[✓] All logs cleaned, normalized, and saved to normalized_logs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447aea24-441d-4979-a97b-3deed8ef916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logs sorted by time saved to logs_sorted_by_time.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('normalized_logs.csv')\n",
    "\n",
    "# Convert 'event_time' to datetime, coercing errors to NaT (Not a Time)\n",
    "# This handles cases where some time entries might not be in a standard format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Drop rows where 'event_time' could not be parsed (NaT values)\n",
    "df.dropna(subset=['timestamp'], inplace=True)\n",
    "\n",
    "# Sort the DataFrame by 'event_time'\n",
    "df_sorted = df.sort_values(by='timestamp')\n",
    "\n",
    "# Save the sorted DataFrame to a new CSV file\n",
    "output_file_name = 'logs_sorted_by_time.csv'\n",
    "df_sorted.to_csv(output_file_name, index=False)\n",
    "\n",
    "print(f\"\\nLogs sorted by time saved to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af62aa1-17d3-4d88-80e3-5204bb4fb96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logs cleaned, renamed, and saved as 'dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"logs_sorted_by_time.csv\")\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Drop empty rows and columns\n",
    "df.dropna(how='all', inplace=True)\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Clean strings (skip these fields)\n",
    "exclude_clean = ['user_agent', 'url', 'fileaccessed', 'message']\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    if col not in exclude_clean:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Convert timestamp\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df = df[df['timestamp'].notna()]\n",
    "\n",
    "# Standardize event IDs\n",
    "for field in ['eventid', 'threat_id']:\n",
    "    if field in df.columns:\n",
    "        df[field] = df[field].replace(['', ' ', 'nan', 'None', None], pd.NA).fillna(-1)\n",
    "        try:\n",
    "            df[field] = df[field].astype(int)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# IP cleanup\n",
    "ip_fields = ['source_ip', 'dest_ip', 'client_ip']\n",
    "for col in ip_fields:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.extract(r'(\\d{1,3}(?:\\.\\d{1,3}){3})')[0].fillna('0.0.0.0')\n",
    "\n",
    "# Port cleanup\n",
    "for port in ['source_port', 'dest_port']:\n",
    "    if port in df.columns:\n",
    "        df[port] = pd.to_numeric(df[port], errors='coerce').fillna(0).astype(int)\n",
    "        df[port] = df[port].replace(0, -1)\n",
    "\n",
    "# Bytes cleanup\n",
    "for col in ['bytes_sent', 'bytes_received']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Normalize categorical values\n",
    "cat_fields = ['action', 'severity', 'verdict', 'log_type', 'category', 'protocol']\n",
    "for col in cat_fields:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace({'none': np.nan, 'na': np.nan, 'null': np.nan}).fillna('unknown')\n",
    "\n",
    "# Fill general missing values\n",
    "df.fillna({\n",
    "    'user': 'unknown',\n",
    "    'app': 'unknown',\n",
    "    'rule_name': 'unknown',\n",
    "    'host': 'unknown',\n",
    "    'workstation': 'unknown',\n",
    "    'privilege': 'unknown',\n",
    "    'event': 'unknown',\n",
    "    'status': 'unknown',\n",
    "    'alert': 'none',\n",
    "    'machine': 'unknown',\n",
    "    'os': 'unknown',\n",
    "    'domain': 'unknown',\n",
    "    'location': 'unknown',\n",
    "    'reason': 'unspecified',\n",
    "    'accessmask': 'unknown',\n",
    "}, inplace=True)\n",
    "\n",
    "# Fill text fields\n",
    "text_fill = {\n",
    "    'url': 'unknown_url',\n",
    "    'user_agent': 'unknown_agent',\n",
    "    'fileaccessed': 'none',\n",
    "    'message': 'no_message',\n",
    "}\n",
    "for col, default in text_fill.items():\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(default).replace('nan', default).replace('', default)\n",
    "\n",
    "# Rename columns to consistent format\n",
    "column_rename_map = {\n",
    "    'timestamp': 'event_time',\n",
    "    'eventid': 'event_id',\n",
    "    'threat_id': 'threat_identifier',\n",
    "    'user': 'user_account',\n",
    "    'severity': 'event_severity',\n",
    "    'log_type': 'log_source_type',\n",
    "    'source_ip': 'source_ip_address',\n",
    "    'dest_ip': 'destination_ip_address',\n",
    "    'client_ip': 'client_ip_address',\n",
    "    'dest_domain': 'destination_domain',\n",
    "    'source_port': 'source_port_number',\n",
    "    'dest_port': 'destination_port_number',\n",
    "    'protocol': 'network_protocol',\n",
    "    'action': 'security_action',\n",
    "    'verdict': 'security_verdict',\n",
    "    'category': 'event_category',\n",
    "    'app': 'application_name',\n",
    "    'rule_name': 'detection_rule',\n",
    "    'bytes_sent': 'bytes_sent_total',\n",
    "    'bytes_received': 'bytes_received_total',\n",
    "    'url': 'resource_url',\n",
    "    'method': 'http_method',\n",
    "    'user_agent': 'user_agent_string',\n",
    "    'description': 'event_description',\n",
    "    'fileaccessed': 'file_accessed_path',\n",
    "    'host': 'host_name',\n",
    "    'workstation': 'workstation_name',\n",
    "    'privilege': 'user_privilege_level',\n",
    "    'object': 'affected_object',\n",
    "    'accessmask': 'access_mask',\n",
    "    'target': 'target_resource',\n",
    "    'status': 'event_status',\n",
    "    'reason': 'status_reason',\n",
    "    'process': 'process_name',\n",
    "    'parentprocess': 'parent_process_name',\n",
    "    'alert': 'alert_type',\n",
    "    'machine': 'machine_name',\n",
    "    'os': 'operating_system',\n",
    "    'event': 'raw_event_data',\n",
    "    'file': 'file_path',\n",
    "    'facility': 'log_facility',\n",
    "    'code': 'status_code',\n",
    "    'message': 'log_message',\n",
    "    'details': 'event_details',\n",
    "    'resource': 'resource_name',\n",
    "    'location': 'event_location',\n",
    "    'domain': 'network_domain'\n",
    "}\n",
    "df.rename(columns=column_rename_map, inplace=True)\n",
    "\n",
    "# Reorder columns logically\n",
    "ordered_columns = [\n",
    "    # Event metadata\n",
    "    'event_time','log_source_type' ,'event_id', 'threat_identifier',\n",
    "\n",
    "    # User/device context\n",
    "    'user_account', 'user_privilege_level', 'host_name', 'workstation_name', 'machine_name', 'operating_system',\n",
    "\n",
    "    # Network context\n",
    "    'source_ip_address', 'source_port_number', 'destination_ip_address', 'destination_port_number',\n",
    "    'client_ip_address', 'destination_domain', 'network_domain', 'network_protocol',\n",
    "\n",
    "    # Security action\n",
    "    'security_action', 'event_severity', 'event_status', 'status_reason', 'event_category', 'detection_rule', 'security_verdict', 'alert_type',\n",
    "\n",
    "    # Resource access\n",
    "    'resource_url', 'http_method', 'user_agent_string', 'resource_name', 'affected_object', 'target_resource',\n",
    "    'file_accessed_path', 'file_path',\n",
    "\n",
    "    # Process/activity\n",
    "    'process_name', 'parent_process_name', 'access_mask',\n",
    "\n",
    "    # Logging & raw data\n",
    "    'event_description', 'event_details', 'log_message', 'raw_event_data', 'log_facility', 'status_code', 'event_location',\n",
    "\n",
    "    # Data volume\n",
    "    'bytes_sent_total', 'bytes_received_total',\n",
    "\n",
    "    # Extras\n",
    "    'application_name'\n",
    "]\n",
    "\n",
    "# Keep only existing columns\n",
    "existing_ordered_columns = [c for c in ordered_columns if c in df.columns]\n",
    "remaining_columns = [c for c in df.columns if c not in existing_ordered_columns]\n",
    "df = df[existing_ordered_columns + remaining_columns]\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save to file\n",
    "df.to_csv(\"dataset.csv\", index=False)\n",
    "print(\"✅ Logs cleaned, renamed, and saved as 'dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef1293d-3629-4c4a-ac77-df0e05512eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134987/1314934964.py:75: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'N/A' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  full_dataset.fillna(\"N/A\", inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./dataset_with_attacks.csv'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load original dataset\n",
    "original_path = \"./dataset.csv\"\n",
    "df = pd.read_csv(original_path)\n",
    "\n",
    "# Helper function to generate random times\n",
    "def random_time(start_time, end_time, count):\n",
    "    start = datetime.strptime(start_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    end = datetime.strptime(end_time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return [start + timedelta(seconds=random.randint(0, int((end - start).total_seconds()))) for _ in range(count)]\n",
    "\n",
    "# Predefined attacks to simulate\n",
    "def generate_attack_logs(num_each=10):\n",
    "    logs = []\n",
    "\n",
    "    phishing_users = ['noura.benali', 'ismail.rahimi', 'yassir.ouattara']\n",
    "    brute_users = ['saif.alqahtani', 'hajar.ait', 'amin.elk']\n",
    "    lateral_users = ['karim.dadi', 'mohamed.hassan', 'nadia.bouk']\n",
    "\n",
    "    times = random_time(\"2025-05-26 09:00:00\", \"2025-05-26 11:00:00\", num_each * 3)\n",
    "\n",
    "    for i in range(num_each):\n",
    "        # Phishing attack\n",
    "        logs.append({\n",
    "            \"event_time\": times[i],\n",
    "            \"log_source_type\": \"firewall\",\n",
    "            \"event_id\": -1,\n",
    "            \"threat_identifier\": \"phishing\",\n",
    "            \"user_account\": random.choice(phishing_users),\n",
    "            \"event_description\": \"User clicked on suspicious email link\",\n",
    "            \"application_name\": \"http\",\n",
    "            \"log_message\": \"Access to known phishing domain detected\",\n",
    "            \"bytes_sent_total\": random.randint(200, 1000),\n",
    "            \"bytes_received_total\": random.randint(1000, 4000),\n",
    "        })\n",
    "\n",
    "        # Brute force attack\n",
    "        logs.append({\n",
    "            \"event_time\": times[i + num_each],\n",
    "            \"log_source_type\": \"edr\",\n",
    "            \"event_id\": -1,\n",
    "            \"threat_identifier\": \"brute_force\",\n",
    "            \"user_account\": random.choice(brute_users),\n",
    "            \"event_description\": \"Multiple failed login attempts\",\n",
    "            \"application_name\": \"ssh\",\n",
    "            \"log_message\": \"Account locked after repeated login failures\",\n",
    "            \"bytes_sent_total\": random.randint(100, 300),\n",
    "            \"bytes_received_total\": random.randint(50, 150),\n",
    "        })\n",
    "\n",
    "        # Lateral movement\n",
    "        logs.append({\n",
    "            \"event_time\": times[i + 2*num_each],\n",
    "            \"log_source_type\": \"ad\",\n",
    "            \"event_id\": -1,\n",
    "            \"threat_identifier\": \"lateral_movement\",\n",
    "            \"user_account\": random.choice(lateral_users),\n",
    "            \"event_description\": \"Suspicious internal remote login\",\n",
    "            \"application_name\": \"rdp\",\n",
    "            \"log_message\": \"Unusual RDP access across subnets\",\n",
    "            \"bytes_sent_total\": random.randint(300, 1200),\n",
    "            \"bytes_received_total\": random.randint(400, 1500),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(logs)\n",
    "\n",
    "# Generate and merge with original data\n",
    "attack_logs_df = generate_attack_logs(num_each=20)\n",
    "full_dataset = pd.concat([df, attack_logs_df], ignore_index=True)\n",
    "\n",
    "# Fill NaNs with a default string to avoid correlation issues\n",
    "full_dataset.fillna(\"N/A\", inplace=True)\n",
    "\n",
    "# Save the result\n",
    "attack_path = \"./dataset_with_attacks.csv\"\n",
    "full_dataset.to_csv(attack_path, index=False)\n",
    "\n",
    "attack_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc0b1a3-bf2a-4cb3-bc8c-2a040a98375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to user_activity_correlation_5min.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"dataset_with_attacks.csv\")\n",
    "df['event_time'] = pd.to_datetime(df['event_time'], errors='coerce')\n",
    "\n",
    "# Create 5-minute window bucket\n",
    "df['time_bucket'] = df['event_time'].dt.floor('5min')\n",
    "\n",
    "# Group by user and time window\n",
    "grouped = df.groupby(['user_account', 'time_bucket'])\n",
    "\n",
    "# List of all events for each group\n",
    "summary = grouped['event_description'].apply(lambda x: list(x.dropna())).reset_index()\n",
    "summary.rename(columns={'event_description': 'events_in_5min'}, inplace=True)\n",
    "\n",
    "# Correlation Rule Logic\n",
    "def classify_threat(events):\n",
    "    events_text = ' '.join(events).lower()\n",
    "    if \"phishing\" in events_text or \"suspicious email\" in events_text:\n",
    "        return \"phishing\"\n",
    "    elif \"failed login\" in events_text or \"account locked\" in events_text:\n",
    "        return \"brute_force\"\n",
    "    elif \"rdp access\" in events_text or \"remote login\" in events_text:\n",
    "        return \"lateral_movement\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "\n",
    "# Apply correlation rule\n",
    "summary['correlated_threat'] = summary['events_in_5min'].apply(classify_threat)\n",
    "\n",
    "# Save the correlation summary\n",
    "summary.to_csv(\"user_activity_correlation_5min.csv\", index=False)\n",
    "print(\"Saved to user_activity_correlation_5min.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e24f8-2d10-4a99-929a-559d4f770ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
